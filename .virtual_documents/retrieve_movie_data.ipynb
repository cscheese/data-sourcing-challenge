


# Dependencies
import requests
import time
from dotenv import load_dotenv
import os
import pandas as pd
import json


# Set environment variables from the .env in the local environment
load_dotenv('.env')

nyt_api_key = os.getenv('NYT')
tmdb_api_key = os.getenv('TMDB')

# I really struggled to get the API's to work. 
#I used AskBCS where they were finally able to help me get this figured out. 






# Set the base URL
url = "https://api.nytimes.com/svc/search/v2/articlesearch.json?"

# Filter for movie reviews with "love" in the headline
# section_name should be "Movies"
# type_of_material should be "Review"
filter_query = 'section_name:"Movies" AND type_of_material:"Review" AND headline:"love"'

# Use a sort filter, sort by newest
sort = "newest"

# Select the following fields to return:
# headline, web_url, snippet, source, keywords, pub_date, byline, word_count
field_list = "headline,web_url,snippet,source,keywords,pub_date,byline,word_count"

# Search for reviews published between a begin and end date
begin_date = "20130101"
end_date = "20230531"

# Build URL
query_url = (
    f"{url}api-key={nyt_api_key}&begin_date={begin_date}&end_date={end_date}"
    + f'&fq={filter_query}&sort={sort}&fl={field_list}'
)


# Print `response_data variable`
response = requests.get(query_url)
response

#checking to see what response I am getting


# Create an empty list to store the reviews
results_list = []

# loop through pages 0-19
for page_number in range(20):   

    # Set up the base query URL
    query_url = f"{url}api-key={nyt_api_key}&begin_date={begin_date}&end_date={end_date}"
   
    # create query with a page number
    query_url_with_page = f'{query_url}&page={page_number}'
       
    # API results show 10 articles at a time
    print(f"Checked page: {page_number}")
    
    # Make a "GET" request and retrieve the JSON
    results = requests.get(query_url_with_page).json()
            
    # Add a twelve second interval between queries to stay within API query limits
    time.sleep(12)
   
    if not results["response"]["docs"]:
        print(f"No results on page {page_number}")
        break  # Break from the loop if no results
    try:
        # If there are results, loop through and save each review
        for review in results["response"]["docs"]:
            results_list.append(review)
    except: 
        # Print the reviews added from the current page
        print(f"Page {page_number} reviews added to list.")

# I used Xpert and ChatGPT and looking at homework to generate this code


# Preview the first 5 results in JSON format
# Use json.dumps with argument indent=4 to format data

for review in results_list[:5]:
    print(json.dumps(results, indent=4))  
# I used Xpert and ChatGPT and looking at homework to generate this code


# Convert reviews_list to a Pandas DataFrame using json_normalize()

import pandas as pd
from pandas import json_normalize

# Convert results_list to a Pandas DataFrame
reviews_df = json_normalize(results_list)

# Display the DataFrame
reviews_df






reviews_df.columns
#I added this additional step so I could see all of the columns


print(reviews_df['headline.main'])
# I added this additional step so I could see what was in the column 'headline.main'


# Extract the title from the "headline.main" column and
# save it to a new column "title"
# Title is between unicode characters \u2018 and \u2019. 
# End string should include " Review" to avoid cutting title early
reviews_df["title"] = reviews_df["headline.main"].apply(lambda st: st[st.find("\u2018")+1:st.find("\u2019 Review")])
reviews_df



print(reviews_df['title'])


reviews_df['keywords']


# Extract 'name' and 'value' from items in "keywords" column
def extract_keywords(keyword_list):
    extracted_keywords = ""
    for item in keyword_list:
        # Extract 'name' and 'value'
        keyword = f"{item['name']}: {item['value']};" 
        # Append the keyword item to the extracted_keywords list
        extracted_keywords += keyword
    return extracted_keywords

# Fix the "keywords" column by converting cells from a list to a string
reviews_df['keywords'] = reviews_df['keywords'].apply(extract_keywords)

reviews_df


# Create a list from the "title" column using to_list()
# These titles will be used in the query for The Movie Database
title_list = reviews_df['title'].to_list()
title_list





# Prepare The Movie Database query
nyt_api_key = os.getenv('NYT')
tmdb_api_key = os.getenv('TMDB')

url = "https://api.themoviedb.org/3/search/movie?query="
tmdb_key_string = "&api_key=" + tmdb_api_key



    # Perform a "GET" request for The Movie Database
reviews = requests.get(author_query_url + author)

    # Include a try clause to search for the full movie details.
    # Use the except clause to print out a statement if a movie
    # is not found.


# Create an empty list to store the results
tmdb_movies_list = []

# Create a request counter to sleep the requests after a multiple
# of 50 requests
request_counter = 0

# Loop through the titles
for title in title_list:
    # Check if we need to sleep before making a request
    if request_counter % 50 == 0 and request_counter != 0:
        time.sleep(1)  # Sleep for 1 second after every 50 requests

    # Increment the request counter
    request_counter += 1

    # Perform a "GET" request to The Movie Database
    results = requests.get(query_url_with_page).json()
    response = requests.get(f"{url}?api_key={tmdb_key_string}&query={title}")
    response_data = response.json()

    try:
        # Get movie ID
        movie_ID = response_data['results'][0]['id']
        
        # Make a request for the full movie details
        details_url = f"https://api.themoviedb.org/3/movie/{movie_ID}?api_key={tmdb_key_string}"
        full_response = requests.get(details_url)
        full_data = full_response.json()

        # Extract relevant details
        genres = [genre['name'] for genre in full_data.get('genres', [])]
        spoken_languages = [lang['english_name'] for lang in full_data.get('spoken_languages', [])]
        production_countries = [country['name'] for country in full_data.get('production_countries', [])]

        # Store the relevant data in a dictionary
        movie_info = {
            "title": title,
            "genres": genres,
            "spoken_languages": spoken_languages,
            "production_countries": production_countries
        }

        # Append the dictionary to the results list
        mov_results_list.append(movie_info)

        # Print out the title that was found
        print(f"Found {title}")

    except (IndexError, KeyError):
        # Handle the case where the movie is not found or there is an issue with the response
        print(f"{title} not found or there was an error retrieving the data.")

# The mov_results_list now contains the extracted movie data



# Create an empty list to store the results
mov_results_list = []

# Create a request counter to sleep the requests after a multiple
# of 50 requests

request_counter = 0

# Loop for making requests
for i in range(100):  # Assuming 100 requests need to be made
    # Make the API request here
    request_counter += 1
    
    if request_counter % 50 == 0:
        time.sleep(1)  # Sleep for 1 second after every 50 requests

# Loop through the titles; title_list
  

    # Check if we need to sleep before making a request


    # Add 1 to the request counter

    
    # Perform a "GET" request for The Movie Database


    # Include a try clause to search for the full movie details.
    # Use the except clause to print out a statement if a movie
    # is not found.
    try:
        # Get movie id
        movie_ID = response_data['results'][0]['id']

        # Make a request for a the full movie details
        response _data = requests.get(url + titles + tmdb_key_string).json()

        # Execute "GET" request with url

        
        # Extract the genre names into a list


        # Extract the spoken_languages' English name into a list


        # Extract the production_countries' name into a list


        # Add the relevant data to a dictionary and
        # append it to the tmdb_movies_list list

        
        # Print out the title that was found
        except:
            print(titles+"not found")



# Preview the first 5 results in JSON format
# Use json.dumps with argument indent=4 to format data


# for review in results_list[:5]:
 #   print(json.dumps(results, indent=4))  



# Convert the results to a DataFrame






# Merge the New York Times reviews and TMDB DataFrames on title



# Remove list brackets and quotation marks on the columns containing lists
# Create a list of the columns that need fixing


# Create a list of characters to remove


# Loop through the list of columns to fix

    # Convert the column to type 'str'


    # Loop through characters to remove


# Display the fixed DataFrame



# Drop "byline.person" column



# Delete duplicate rows and reset index



# Export data to CSV without the index




